#### How to choose a proper activation function?
- Use Relu non-linearity. Be careful with learning rate and possibly the fraction of dead units in the network. If it happens, try Tanh / Maxout


